arrange(desc(median)) %>%
mutate(sum_n = cumsum(n),
ZipGroup = ntile(sum_n, 3))
hmod1 <- lm(AdjSalePrice ~ SqFtTotLiving + BldgGrade, houses) # our first model
install.packages("plotly")
houses <- read.csv("https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/data/house_sales.csv", sep = "\t")
hmod1 <- lm(AdjSalePrice ~ SqFtTotLiving + BldgGrade, houses) # our first model
zip_groups <- houses %>% # our dplyr code for creating a 3-level grouping for zipcode
mutate(resid = resid(hmod1)) %>%
group_by(ZipCode) %>%
summarise(median = median(resid),
n = n()) %>%
arrange(desc(median)) %>%
mutate(sum_n = cumsum(n),
ZipGroup = ntile(sum_n, 3))
houses <- houses %>% # joining the zipcode groups to our original dataset
left_join(select(zip_groups, ZipCode, ZipGroup), by = "ZipCode")
hmod2 <- lm(AdjSalePrice ~ SqFtTotLiving + BldgGrade + as.factor(ZipGroup), houses) # new model with location included
hmod_bv <- lm(AdjSalePrice ~ SqFtTotLiving, data = houses) # bivariate regression model
hmod3 <- lm(AdjSalePrice ~ poly(SqFtTotLiving, 2) # use poly() function
+ BldgGrade + as.factor(ZipGroup), data = houses)
stargazer(hmod1, hmod2, hmod3, type = "html")
hmod3
hmod2
hmod3 <- lm(AdjSalePrice ~ SqFtTotLiving^2 # use poly() function
+ BldgGrade + as.factor(ZipGroup), data = houses)
hmod3
hmod3 <- lm(AdjSalePrice ~ poly(SqFtTotLiving, 2) # use poly() function
+ BldgGrade + as.factor(ZipGroup), data = houses)
hmod3
houses$SqFtTotLiving
930^2
2150^2
3320^2
?houses
houses$sqft <- houses$SqFtTotLiving^2
hmod3 <- lm(AdjSalePrice ~ sqft # use poly() function
+ BldgGrade + as.factor(ZipGroup), data = houses)
hmod3
qt(.025, df=1160-3-1)
inv.logit(2.15)
library(boot)
inv.logit(2.15)
exp(0.8956688)
exp(2.15)/(1+exp(2.15)
)
exp(0.8956688)
exp(.89)
exp(.91)
exp(.95)
exp(.8)
exp(.7)
exp(.75)
exp(.79)
exp(.77)
exp(1.73)
inv.logit(1.73)
log(1.73)
log(2.15)
exp(0.7654678)
1.55/pt(.025, df=844-8-2)
pt(.025, df=844-8-2)
pt(.025, df=844-8-2)*3
pt(.025, df=844-8-2)*.3
2.15+(pt(.025, df=844-8-2)*.3)
pt(.025, df=844-8-2)
sqrt(844)
.3/sqrt(844)
.5*.01
.5/sqrt(844)
.5*(0.01721071/sqrt(844))
.5*(0.1721071/sqrt(844))
.5*(1.721071/sqrt(844))
.5*(7.21071/sqrt(844))
.5*(log(0.01721071)/sqrt(844))
.5*(log(0.1721071)/sqrt(844))
.5*(log(0.2721071)/sqrt(844))
.5*(exo(0.01721071)/sqrt(844))
.5*(exp(0.01721071)/sqrt(844))
.5*(exp(0.1721071)/sqrt(844))
# take odds and CIs in paper, reverse engineer
log(2.15)
# get coef = 0.7654678 (log odds ratio)
# now need SE, so need p
# p = exp(beta)/(1+exp(beta))
exp(0.7654678)/(1+exp(0.7654678))
# get coef = 0.7654678 (log odds ratio)
# now need SE
qt(0.025/2, df = 844-9-1-1) # Calculating the t-value using quantile function
# get coef = 0.7654678 (log odds ratio)
# now need SE
qt(0.05/2, df = 844-9-1-1) # Calculating the t-value using quantile function
# get coef = 0.7654678 (log odds ratio)
# now need SE
2.15/abs(qt(0.05/2, df = 844-9-1-1) )
abs(qt(0.05/2, df = 844-9-1-1)
)
2.15+1.96
log(2.97)
install.packages('RStata')
library(RStata)
stata("Dropbox/Trinity/projects/FrenchCCProject/drafts/CompLawRevisionPlan/COMPLAW2020PAPER/archive/2020draftandanalysis/my_dofile.do")
?stata
stata("Dropbox/Trinity/projects/FrenchCCProject/drafts/CompLawRevisionPlan/COMPLAW2020PAPER/archive/2020draftandanalysis/my_dofile.do", stata.version=14)
%    349+1+20
%    75+4+0
%    175+2+84
%    341+3+20
%    651+11+15
%    293+8+0
%    613+5+232
%    724+5+149
%    280+1+0
349+1+20
75+4+0
175+2+84
341+3+20
651+11+15
293+8+0
613+5+232
724+5+149
280+1+0
370 +
79+
261 +
364+
677+
301+
850+
878+
281
192+1+23
+
75+4+0 +
175+2+84 +
341+3+20 +
651+11+15 +
296+8+0+
613+5+232+
724+5+149+
280+1+0
370 +
79+
261 +
364+
677+
301+
850+
878+
281
349+1+20
75+4+0
175+2+84
341+3+20
651+11+15
293+8+0
613+5+232
724+5+149
280+1+0
349+1+20+
75+4+0 +
175+2+84+
341+3+20 +
651+11+15+
293+8+0 +
613+5+232+
724+5+149 +
280+1+0
370 +
79+
261 +
364+
677+
301+
850+
878+
281
192+1+23
192+1+23 +
75+4+0 +
175+2+84 +
341+3+20 +
651+11+15 +
296+8+0+
613+5+232+
724+5+149+
280+1+0
308+1+20
308+1+20+43+4+0 +
175+2+84
308+1+20+
43+4+0 +
175+2+84 +
341+3+20 +
651+11+15+
296+8+46 +
613+5+160+
757+5+140+280+1+0
set . seed (123)
data <− data.frame(x = runif(200, 1, 10)) data$y <− 0 + 2.75∗data$x + rnorm(200, 0, 1.5)
set.seed(123)
data <-data.frame(x = runif(200, 1, 100))
data$y <- 0 + 2.75*data$x +rnorm(200, 0, 1.5)
# write the function of the OLS regression with θ=(β, σ)
linear.lik <- function(theta, y, x){
n <- nrow(x)
k <- ncol(x)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y-x%*%beta
logl <- -0.5*n*log(2*pi)-0.5*n*log(sigma2)-((t(e)%*%e)/(2*sigma2))
return(-logl)
}
linear.MLE <- optim(fn=linear.lik, par = c(1,1,1), hessian = TRUE, y = data$y, x = cbind(1, data$x), method = "BFGS")
linear.MLE$par
?data("infants")
df_s2 <- data.frame(out)
> View(df_s2)
> plot(df_s1)
> out = tab_Q_summary %>%
+   arrange(outcome) %>%
+   mutate(Correct = recode(corr_w1, `-1`="All", `0`="Incorrect", `1`="Correct", `3`="zDinD"),
+          column = paste(outcome, Correct, sep = "_")
+   ) %>%
+   select(-(statistic:corr_w1), -term, -Correct) %>%
+   gather(variable, value, estimate, std.error) %>%
+   mutate(
+     value = format_num(value, 3),
+     value = ifelse(variable == "std.error", paste0("(", value, ")"), value),
+     Congenial = ifelse(is.na(Congenial), "All", as.character(Congenial))
+   ) %>%
+   spread(column, value) %>%
+   select(-variable) %>%
+   select(Category, Congenial, corr_w1_All, cert_w1_Correct, pInitial_cross_Correct, zDiff_Correct, cert_w1_Incorrect, pInitial_cross_Incorrect, zDiff_Incorrect, zDiD_zDinD) %>%
+   mutate(Category = as.character(Category))
>
>
> for(i in nrow(out):2){
+   if(out$Category[i]==out$Category[i-1]) out$Category[i] = ""
+ }
>
> for(i in ((1:(nrow(out)/2))*2-1)){
+   if(out$Congenial[i] == "All"){
+     out$Congenial[i] = "All responses"
+     out$Congenial[i+1] = ""
+   }
+   if(out$Congenial[i] == "Congenial"){
+     out$Congenial[i] = "Correct ans."
+     out$Congenial[i+1] = "is congenial"
+   }
+   if(out$Congenial[i] == "Uncongenial"){
+     out$Congenial[i] = "Incorrect ans."
+     out$Congenial[i+1] = "is congenial"
+   }
+   if(out$Category[i] == "Political knowledge"){
+     out$Category[i] = "Political"
+     out$Category[i+1] = "knowledge"
+   }
+   if(out$Category[i] == "Controversies"){
+     out$Category[i] = "Contro-"
+     out$Category[i+1] = "versies"
+   }
+ }
> View(out)
> View(df_s2)
> out = tab_cross_stabil %>%
+   group_by()  %>%
+   filter(!is.na(Congenial),
+          variable == "pInitial_cross") %>%
+   group_by() %>%
+   left_join(tab_cross_stabil_N %>% filter(
+     variable == "pInitial_cross")
+   ) %>%
+   filter(Date == "March-August 2020") %>%
+   mutate(
+     N = ifelse(is.na(N), 0, N)
+   ) %>%
+   mutate(
+     CI   = paste0("(", format_num(conf.low), ", ", format_num(conf.high), ")"),
+     N = as.character(round(N))
+   ) %>%
+   select(Survey, Category, Congenial, Correct, Certainty=cert_bin_w1, Estimate=estimate, SE=std.error, CI, N)  %>%
+   arrange(Survey, Category, Congenial, Correct, Certainty) %>%
+   mutate(Certainty = recode(Certainty, `0.5`="0.5", `0.55`="[0.51,0.59]", `0.65`="[0.6,0.69]", `0.75`="[0.7,0.79]", `0.85`="[0.8,0.89]", `0.95`="[0.9,0.99]", `1`="1"),
+          Survey = gsub("MTurk, ", "", Survey),
+          Category = ifelse(Congenial == "Political knowledge", "Political knowledge", Category),
+          Congenial = recode(Congenial, `Political knowledge`="Not applicable"))
df_s1 <- data.frame(tab_var$cert_bin_w1, tab_var$estimate)
df_s2 <- data.frame(s2$Certainty, s2$Estimate)
df_s2["2", "s2.Certainty"] <- 0.55
df_s2["3", "s2.Certainty"] <- 0.65
df_s2["4", "s2.Certainty"] <- 0.75
df_s2["5", "s2.Certainty"] <- 0.85
df_s2["6", "s2.Certainty"] <- 0.95
df_s2["9", "s2.Certainty"] <- 0.55
df_s2["10", "s2.Certainty"] <- 0.65
df_s2["11", "s2.Certainty"] <- 0.75
df_s2["12", "s2.Certainty"] <- 0.85
df_s2["13", "s2.Certainty"] <- 0.95
lm_st <- lm(df_s2$s2.Certainty ~ df_s2$s2.Estimate)
lm_s1 <- lm(df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate)
abline(lm(df_s2$s2.Certainty ~ df_s2$s2.Estimate))
abline(lm(df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate))
plot(lm(df_s2$s2.Certainty ~ df_s2$s2.Estimate))
plot(lm(df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate))
summary(lm_st)
lm_s1 <- lm(formula = df_s1$tab_var.cert_bin_w1 ~ df_s1$tab_var.estimate)
lm_s2 <- lm(formula = df_s2$s2.Certainty ~ df_s2$s2.Estimate)
t.test(lm_s1, lm_st, paired = TRUE)
mean(df_s1$tab_var.cert_bin_w1)
mean(df_s2$s2.Certainty)
library(gplots)
plotmeans(tab_var.cert_bin_w1 ~ tab_var.estimate, data = df_s1, frame = FALSE)
plotmeans(s2.Certainty ~ s2.Estimate, data = df_s2, frame= FALSE)
df_s2$s2.Certainty <- as.numeric(df_s2$s2.Certainty)
ggplot(anova(lm_s2, lm_s1))
newvalsforx <- function(x) {
xrng <- seq(min(x), max(x), length.out=100)
function(m) data.frame(x=xrng, y=predict(m, data.frame(x=xrng)))
}
ggplot(d, aes("Expected", "Certainty")) +
geom_point() +
geom_line(data= lm_st, color="red") +
geom_line(data= lm_s1, color="blue")
ggplot(df_s2, aes(lm_s1, lm_s2))
table(df_s1)
abline(lm(lm_s1))
x1 <- df_s1$tab_var.cert_bin_w1
y1 <- df_s1$tab_var.estimate
plot(x1, y1, main = "Respondent Certitude_Incentivized",
xlab = "Certainty", ylab = "Estimate",
pch = 19, frame = FALSE)
abline(lm(y1 ~ x1, data = df_s1), col = "blue")
x2 <- df_s2$s2.Certainty
y2 <- df_s2$s2.Estimate
plot(x2, y2, main = "Respondent Certitude_2-Wave",
xlab = "Certainty", ylab = "Estimate",
pch = 19, frame = FALSE)
abline(lm(y2 ~ x2, data = df_s2), col = "blue")
x <- rnorm(100, 0, 1)
rm(x)
x <- rnorm(100, 0, 1)
?merge
?mean
x <- rnorm(100, 0, 1)
y <- rnorm(100, 0, 1)
plot(x, y)
77001 23 +
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("ggplot2"),  pkgTest)
# set working directory to current parent folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data as vector
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# capture the number of observations
n <- length(y)
# (a) Calculate the 90% confidence interval for the student IQ
# Step 1: get t-score
t <- qt(0.05, n-1, lower.tail = F)
t
qt(0.05,n−1,lower.tail=F)
pnorm(98, mean = 100, sd = se, lower.tail = FALSE)
rm(list=ls())
#install.packages("tidyverse")
library("tidyverse")
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
summary(y)
sd <- sd(y)
sd
y.bar <- mean(y)
y.bar
se <- sd(y)/sqrt(length(y))
se
z.score <- (y.bar - 100) / se
pnorm(z.score, lower.tail = FALSE)
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108,
87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
meanIQ_Scores}
IQ_Scores <- y
tracemem(IQ_Scores)
rm(y)
n <- 25
length(IQ_Scores)
##Find sum of scores
sum(IQ_Scores)
sumIQ_Scores = sum(IQ_Scores)
##Find sample mean
mean(IQ_Scores)
meanIQ_Scores = mean(IQ_Scores)
mean(IQ_Scores)
sumIQ_Scores = function(IQ_Scores){IQ_Scores-
square_Observation_Mean =
Observation_Mean*Observation_Mean
sum_square_Observation_Mean =
sum(square_Observation_Mean)
standard_deviation =
sqrt(sum_square_Observation_Mean/24)
standard_deviation
##Find sample mean
mean(IQ_Scores)
meanIQ_Scores = mean(IQ_Scores)
mean(IQ_Scores)
sumIQ_Scores = function(IQ_Scores){IQ_Scores-meanIQ_Scores}
Observation_Mean = sumIQ_Scores(IQ_Scores)
square_Observation_Mean =
Observation_Mean*Observation_Mean
sum_square_Observation_Mean =
sum(square_Observation_Mean)
standard_deviation =
sqrt(sum_square_Observation_Mean/24)
standard_deviation
sd(y)
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108,
87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
meanIQ_Scores}
IQ_Scores <- y
tracemem(IQ_Scores)
rm(y)
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108,
87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
sd(y)
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108,
87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
meanIQ_Scores}
IQ_Scores <- y
library(tinytex) help("tinytex")
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
M <- mean(y)
length(y)
s <- sd(y)
sq <- sqrt(25)
Upper_end_90 <- M + 1.68*(s/sq) Lower_end_90 <- M - 1.68*(s/sq) M
Lower_end_90 Upper_end_90
sq <- sqrt(25)
Upper_end_90 <- M + 1.68*(s/sq)
Lower_end_90 <- M - 1.68*(s/sq) M
library(tinytex)
help("tinytex")
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113,
112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
M <- mean(y)
length(y)
s <- sd(y)
sq <- sqrt(25)
Upper_end_90 <- M + 1.68*(s/sq)
Lower_end_90 <- M - 1.68*(s/sq)
M
Lower_end_90
Upper_end_90
# load data as vector
y <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94, 113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
# capture the number of observations
n <- length(y)
# (a) Calculate the 90% confidence interval for the student IQ
# Step 1: get t-score
t <- qt(0.05, n-1, lower.tail = F)
t
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
install.packages('tidyverse')
install.packages("tidyverse")
IQ <- c(105, 69, 86, 100, 82, 111, 104, 110, 87, 108, 87, 90, 94,
113, 112, 98, 80, 97, 95, 111, 114, 89, 95, 126, 98)
plot(density(IQ), main = "Density Plot of Student IQ Sample")
n_IQ <- length(IQ)
mean_IQ <- sum(IQ)/length(IQ)
sq_diff <- vector("double", length = length(IQ))
for (i in seq_len(length(IQ))) {
sq_diff[i] <- (IQ[i]-mean_IQ)^2
}
sd_IQ <- sqrt((sum(sq_diff))/(n_IQ - 1))
#Calculate standard error and save as object
se_IQ <- sd_IQ / sqrt(n_IQ)
tscore_IQ <- qt(0.95, 24, lower.tail = TRUE)
tscore_IQ
MOE <- tscore_IQ * se_IQ
MOE
CI_lower <- mean_IQ - MOE
CI_upper <- mean_IQ + MOE
CI_lower
CI_upper
